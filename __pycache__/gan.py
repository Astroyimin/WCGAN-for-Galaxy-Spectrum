# -*- coding: utf-8 -*-
"""gan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVEl0jGOFZ9Lyu8DEjOkP9Blk1Eu61Mn
"""

import sys

import math
import numpy as np
import torch
import torch.nn as nn

import matplotlib.pyplot as plt
import h5py

from torchinfo import summary

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

def min_max_norm(x):
    x = (x-np.min(x))/(np.max(x)-np.min(x))
    return(x)

### dataset
f_data = h5py.File("sdss_galaxy_spec_align.hdf5", "r")
specs_low_res = f_data['processed']['specs'][()]
wavelength_low_res = f_data['processed']['wavelength'][()]
# print(specs_low_res.shape)
z = f_data['processed']['z'][()]
age = f_data['processed']['age'][()]
metallicity = f_data['processed']['metallicity'][()]
smass = f_data['processed']['smass'][()]
# Condition
condition = np.stack((min_max_norm(age),
                        min_max_norm(metallicity),
                        min_max_norm(smass),
                        min_max_norm(z)
                        ), axis=1)
# normalization
normalized_data = specs_low_res - np.min(specs_low_res) + 1
normalized_data = np.log10(normalized_data)
min_val, max_val = np.min(normalized_data), np.max(normalized_data)
normalized_data = (normalized_data - min_val) / (max_val - min_val)

# divide into training / validation / test data
ndata = len(specs_low_res)
ndata_train = int(ndata*0.9)
ndata_val = 32

training_data = normalized_data[:ndata_train-ndata_val]
val_data = normalized_data[ndata_train-ndata_val:ndata_train]
test_data = normalized_data[ndata_train:]

training_par = condition[:ndata_train-ndata_val]
val_par = condition[ndata_train-ndata_val:ndata_train]
test_par = condition[ndata_train:]

# convert to torch tensor
training_data = torch.from_numpy(training_data.astype(np.float32))
val_data = torch.from_numpy(val_data.astype(np.float32)).to(device)
test_data = torch.from_numpy(test_data.astype(np.float32)).to(device)

# convert to torch tensor
training_par = torch.from_numpy(training_par.astype(np.float32))
val_par = torch.from_numpy(val_par.astype(np.float32)).to(device)
test_par = torch.from_numpy(test_par.astype(np.float32)).to(device)

print("training data size: ", training_data.size())
print('training condition size :',training_par.shape)
print("validation data size: ", val_data.size())
print("test data size: ", test_data.size())

## GAN model
import ResNet
class Generator(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super().__init__()
        self.pre=nn.Conv1d(1, 3, kernel_size=1, stride=1,
                     padding=1, bias=False)
        self.Residual = ResNet.restnet18gcbam(numberclass=hidden_dim,cbam = True,linear=True)
        self.Dropout = nn.Dropout(0.1)
        self.premodel = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim//4),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim//4, hidden_dim//4),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim//4, hidden_dim//2),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//2),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//2),
            nn.BatchNorm1d(hidden_dim//2),
            nn.LeakyReLU(),
            # nn.BatchNorm1d(hidden_dim),
            # nn.LeakyReLU(),
            
        )
        self.model = nn.Sequential(
            nn.Linear(hidden_dim, output_dim),
            nn.LeakyReLU(),
            nn.Linear(output_dim, output_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.premodel(x)
        x = x.unsqueeze(dim=1)
        x = self.pre(x)
        x = self.Residual(x)
        x = self.Dropout(x)
        x = self.model(x)
        return x

class Discriminator(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.premodel = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU()
        )
        self.Dropout = nn.Dropout(0.2)
        self.Residual = ResNet.restnet18cbam(numberclass=hidden_dim//2,cbam = True,linear=True)
        self.model = nn.Sequential(
            nn.Linear(hidden_dim//2, hidden_dim//2),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim//2, 1),
        )
        self.pre=nn.Conv1d(1, 3, kernel_size=1, stride=1,
                     padding=1, bias=False)
    def forward(self, x):
        x = self.premodel(x)
        x = x.unsqueeze(dim=1)
        x = self.pre(x)
        x = self.Residual(x)
        x = self.Dropout (x)
        x = self.model(x)
        return x

latent_dim = 4
generator = Generator(
    latent_dim = latent_dim,
    hidden_dim = 200,
    output_dim = specs_low_res.shape[-1]
    )
discriminator = Discriminator(
    input_dim = specs_low_res.shape[-1],
    hidden_dim = 200
)

generator.to(device)
discriminator.to(device)

batch_size = 32
summary(generator, input_size=(batch_size, latent_dim), col_names = ["input_size", "output_size", "num_params"])
batch_size = 32
summary(discriminator, input_size=(batch_size, training_data.size(-1)), col_names = ["input_size", "output_size", "num_params"])
### Train GAN
### Train GAN
criterion = nn.BCELoss()
criterion_mse = nn.MSELoss()

optimizer_g = torch.optim.RMSprop(generator.parameters(), lr=0.0002)
optimizer_d = torch.optim.RMSprop(discriminator.parameters(), lr=0.0001)

num_epochs = 100
batch_size = 100
num_batchs = ndata_train // batch_size - 1

history = {"loss_D_real": [], "loss_D_fake": [], "loss_G": []}
for epoch in range(num_epochs):
  indices = torch.randperm(ndata_train-ndata_val)
  training_data = training_data[indices]

  for i in range(num_batchs):
    # real_labels = torch.ones((batch_size, 1)).to(device)
    # fake_labels = torch.zeros((batch_size, 1)).to(device)

    ## Real samples
    real_samples = training_data[batch_size * i : batch_size * (i+1)].to(device)
    pars = training_par[batch_size * i : batch_size * (i+1)].to(device)
    # ## Fake samples
    # noise = torch.randn(batch_size, latent_dim).to(device)
    fake_samples = generator(pars)
    optimizer_d.zero_grad()
    loss_D  = -torch.mean(discriminator(real_samples))+torch.mean(discriminator(fake_samples.detach()))
    loss_D.backward()
    optimizer_d.step()
    for p in discriminator.parameters():
       p.data.clamp_(-0.1,0.1)
    ## Train discriminator
    # if i % 10 == 0:
    # optimizer_d.zero_grad()
    # real_output = discriminator(real_samples)
    # fake_output = discriminator(fake_samples.detach())
    # real_loss = criterion(real_output, real_labels)
    # fake_loss = criterion(fake_output, fake_labels)
    # loss_d = real_loss + fake_loss

    # Train generator
    if i % 5 ==0 :
      optimizer_g.zero_grad()
      gen_sp = generator(pars)
      # fake_output = discriminator(fake_samples)
      # loss_g = criterion(fake_output, real_labels)+0.5*criterion_mse(fake_samples,real_samples)
      loss_g = -torch.mean(discriminator(gen_sp))
      loss_g.backward()
      optimizer_g.step()

    # history["loss_D_real"].append(real_loss.item())
    # history["loss_D_fake"].append(fake_loss.item())
    
    history["loss_D_real"].append(loss_D.item())
    history["loss_G"].append(loss_g.item())
    if epoch%10 ==0 :
       torch.save(generator.state_dict(), f'epoch{epoch}')
  print(f"Epoch {epoch} | Loss D: {loss_D.item()} | Loss G: {loss_g.item()}")

plt.figure()
x = np.linspace(0, len(history["loss_D_real"]), len(history["loss_D_real"]))
plt.plot(x, history["loss_D_real"], label = "D loss")
plt.plot(x, history["loss_G"], label = "G loss")
plt.legend()
plt.savefig('./IMAGE/loss.png')
plt.close()

plt.figure()
plt.plot(x, history["loss_D_real"], label = "D loss")
plt.legend()
plt.savefig('./IMAGE/loss_D_real.png')
plt.close()

plt.figure()
plt.plot(x, history["loss_G"], label = "loss_G")
plt.legend()
plt.savefig('./IMAGE/loss_G.png')
plt.close()



indices = torch.randperm(val_par.size(0))[:10]

random_selection_lb = val_par[indices].to(device)
random_selection_sp = val_data[indices].to(device)

generator.eval()
with torch.no_grad():
  fake_samples = generator(random_selection_lb)

fake_samples = fake_samples.to('cpu').detach().numpy()

plt.figure()
for f in fake_samples:
  plt.plot(wavelength_low_res, f)
plt.title('GEN')
plt.savefig('./IMAGE/fake.png')
plt.close()

random_selection_sp = random_selection_sp.to('cpu').detach().numpy()
plt.figure()
for f in random_selection_sp:
  plt.plot(wavelength_low_res, f)
plt.title('REAL')
plt.savefig('./IMAGE/real.png')
plt.close()

